{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8e1c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:11:40.296614Z",
     "start_time": "2025-04-17T11:11:28.473266Z"
    },
    "id": "1f8e1c7d"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f61d2a-a53b-4e37-b118-927bb59d346b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:12:01.932289Z",
     "start_time": "2025-04-17T11:11:54.355043Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3778,
     "status": "ok",
     "timestamp": 1744450083606,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     },
     "user_tz": -540
    },
    "id": "c5f61d2a-a53b-4e37-b118-927bb59d346b",
    "outputId": "2495b110-0aac-4fc5-f067-f8b5f79106ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/chohi/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/chohi/Korpora/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a11418e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:14:04.559695Z",
     "start_time": "2025-04-17T11:14:04.441896Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1744450204336,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     },
     "user_tz": -540
    },
    "id": "0a11418e",
    "outputId": "180dc9cc-8c1c-4e26-8a41-49befb260b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 40000\n",
      "Testing Data Size : 10000\n"
     ]
    }
   ],
   "source": [
    "train = corpus_df.sample(frac=0.8, random_state=42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71519fa4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-17T11:15:42.081056Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102941,
     "status": "ok",
     "timestamp": 1744450310401,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     },
     "user_tz": -540
    },
    "id": "71519fa4",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "2bb136e0-cfb3-4915-a3db-1cef7f972341"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: A restricted method in java.lang.System has been called\n",
      "WARNING: java.lang.System::load has been called by org.jpype.JPypeContext in an unnamed module (file:/opt/anaconda3/lib/python3.11/site-packages/org.jpype.jar)\n",
      "WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\n",
      "WARNING: Restricted methods will be blocked in a future release unless native access is enabled\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "it_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a1c0ab-dd85-4f97-b013-c1e180141f83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1744451674176,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     },
     "user_tz": -540
    },
    "id": "f0a1c0ab-dd85-4f97-b013-c1e180141f83",
    "outputId": "b594ff3d-764e-4b80-9721-a28e5c66e65b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 222 1656   10 4117 2060  195  775    4    2 2180 1034  221   27   13\n",
      " 4747    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[  78  179    3   30   12   16   49 4816 1578   93   26 1104  805   14\n",
      "   59  912 2523    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in tokens] for tokens in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in tokens] for tokens in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20b6525-cd9a-4d29-a116-05936a1e92ed",
   "metadata": {
    "id": "e20b6525-cd9a-4d29-a116-05936a1e92ed"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866ad18f-31e7-4480-8460-d0b3e0c8ad41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1744451687675,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     },
     "user_tz": -540
    },
    "id": "866ad18f-31e7-4480-8460-d0b3e0c8ad41",
    "outputId": "30b657dd-f8dd-4f77-99bf-ab9b94335568"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8l/dq9nhgmx5gj01xrwhwzb41s40000gn/T/ipykernel_2274/2284756583.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "/var/folders/8l/dq9nhgmx5gj01xrwhwzb41s40000gn/T/ipykernel_2274/2284756583.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_ids = torch.tensor(test_ids)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3994b8de-7656-42b9-b191-9bceaddd9627",
   "metadata": {
    "id": "3994b8de-7656-42b9-b191-9bceaddd9627"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9227be-a546-4157-b472-c6d66d6897ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58093,
     "status": "ok",
     "timestamp": 1744451764494,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     },
     "user_tz": -540
    },
    "id": "dc9227be-a546-4157-b472-c6d66d6897ec",
    "outputId": "84c40c14-8755-4aad-ca7a-328c24d211e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6935114860534668\n",
      "Train Loss 500 : 0.6940433017745942\n",
      "Train Loss 1000 : 0.6921349679911649\n",
      "Train Loss 1500 : 0.6752338931609757\n",
      "Train Loss 2000 : 0.6655980927803586\n",
      "Val Loss : 0.614999490737915, Val Accuracy : 0.6898\n",
      "Train Loss 0 : 0.5591087341308594\n",
      "Train Loss 500 : 0.6021300631606888\n",
      "Train Loss 1000 : 0.5952711787733522\n",
      "Train Loss 1500 : 0.5896994045660704\n",
      "Train Loss 2000 : 0.5855682253360986\n",
      "Val Loss : 0.5450612523317337, Val Accuracy : 0.7482\n",
      "Train Loss 0 : 0.3936803340911865\n",
      "Train Loss 500 : 0.5285916472682933\n",
      "Train Loss 1000 : 0.5057444720120577\n",
      "Train Loss 1500 : 0.4857535206878447\n",
      "Train Loss 2000 : 0.476097430693156\n",
      "Val Loss : 0.4332598125934601, Val Accuracy : 0.7931\n",
      "Train Loss 0 : 0.28045761585235596\n",
      "Train Loss 500 : 0.3959362318891727\n",
      "Train Loss 1000 : 0.40149475481513736\n",
      "Train Loss 1500 : 0.39852927363093577\n",
      "Train Loss 2000 : 0.39484006813217676\n",
      "Val Loss : 0.40441721575260164, Val Accuracy : 0.8078\n",
      "Train Loss 0 : 0.5713988542556763\n",
      "Train Loss 500 : 0.34671418834291773\n",
      "Train Loss 1000 : 0.344803607882141\n",
      "Train Loss 1500 : 0.348238631968773\n",
      "Train Loss 2000 : 0.34992882606016285\n",
      "Val Loss : 0.39442901923656465, Val Accuracy : 0.8175\n"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee7ee60e-31ca-4722-85d1-00c1d48bc66a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1744451782005,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     },
     "user_tz": -540
    },
    "id": "ee7ee60e-31ca-4722-85d1-00c1d48bc66a",
    "outputId": "72fb341a-a1ad-4ca8-a846-9a08f95ac7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한다는 [ 1.2134264   0.5083241  -0.05955544  0.5039435   0.46452317 -1.1349784\n",
      " -0.8432012   0.81685483 -0.15781     0.6552675  -1.39528     0.08630828\n",
      " -0.5595231  -1.2805479   1.2807978   0.7082284   1.2096199  -1.2121688\n",
      " -0.41063595  0.29900295  0.20657925 -0.5033315  -0.02396201 -1.1422771\n",
      "  0.9697187  -0.15655889  0.17611203  0.0907585   0.11308429  1.5620909\n",
      "  0.2549551   1.6331961  -0.6603965   0.3235524   0.634614    0.14647089\n",
      " -0.7273578  -0.11135563  0.13140619 -0.33168113 -1.5377861  -0.3445415\n",
      " -0.72984445  0.22865206  0.7567359   0.30708665  0.13038443  0.22799864\n",
      " -0.5092769   0.38801357  0.4047905   1.6784046   0.45518315  1.1499872\n",
      "  1.2431194   0.16201127 -0.38210467  0.6118739  -0.43785626  0.11033168\n",
      " -0.9882341   0.2815075  -0.9324034  -0.8375494  -1.0080254   1.4601178\n",
      " -2.3658986  -0.21453534 -0.19619988 -0.0816611   0.36210197  1.5280108\n",
      " -0.46419904 -1.5751922   0.2233997  -0.66682327  0.58130664 -2.1452093\n",
      " -2.030154   -0.57062584  1.0495039  -1.5092924   0.16620636  0.19860755\n",
      " -0.7080501   0.90111846  0.19967864  0.7598397   0.82859814 -0.02536582\n",
      " -1.6380305   0.69847214  0.14334238 -2.4185069  -0.2549241   2.2537587\n",
      " -0.18148431 -1.1577792  -0.90270025  0.8225134   0.22618799  1.4658616\n",
      " -2.27316    -1.5720474   0.8896195  -0.22120708  1.3292598  -0.4121554\n",
      " -1.1084075   0.7450338   1.2813711  -1.5369046  -1.4821763  -1.5652381\n",
      " -0.8516577   0.5546903  -0.03927873 -0.471408   -0.42447656 -0.92501193\n",
      " -0.98125213  1.9317709  -0.8771911  -0.55519384 -0.42884398  0.65526956\n",
      " -1.2214905  -0.00822739]\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embbbing_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embbbing_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1t1iYjLxS1JREsWKJNDMRTZO492XxhIg9",
     "timestamp": 1744810295265
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
