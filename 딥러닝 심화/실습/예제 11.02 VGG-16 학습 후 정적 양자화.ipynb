{
 "cells": [
  {
   "cell_type": "code",
   "id": "f49f0bd8-df62-4097-9d3e-9378e48c8e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T06:25:20.291101Z",
     "start_time": "2025-03-29T06:25:15.028328Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.ao import quantization\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "class QuantizedVGG16(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedVGG16, self).__init__()\n",
    "        self.quant = quantization.QuantStub()\n",
    "        self.dequant = quantization.DeQuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "hyperparams = {\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"epochs\": 5,\n",
    "    \"transform\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(128),\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48235, 0.45882, 0.40784],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "model = models.vgg16(num_classes=2)\n",
    "model.load_state_dict(torch.load(\"../models/VGG16.pt\"))\n",
    "#model.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else \"cpu\"\n",
    "\n",
    "#torch.backends.quantized.engine = 'qnnpack'\n",
    "#device = torch.device(\"cpu\")\n",
    "quantized_model = QuantizedVGG16(model).to(device)\n",
    "\n",
    "quantization_backend = \"fbgemm\"\n",
    "quantized_model.qconfig = quantization.get_default_qconfig(quantization_backend)\n",
    "\n",
    "model_static_quantized = quantization.prepare(quantized_model)\n",
    "\n",
    "calibration_dataset = ImageFolder(\n",
    "    \"../datasets/pet/test\",\n",
    "    transform=hyperparams[\"transform\"]\n",
    ")\n",
    "calibration_dataloader = DataLoader(\n",
    "    calibration_dataset,\n",
    "    batch_size=hyperparams[\"batch_size\"]\n",
    ")\n",
    "\n",
    "model_static_quantized.to(\"cpu\")\n",
    "\n",
    "for i, (images, target) in enumerate(calibration_dataloader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    model_static_quantized(images.to(device))\n",
    "\n",
    "model_static_quantized.to(\"cpu\")\n",
    "model_static_quantized = quantization.convert(model_static_quantized)\n",
    "\n",
    "torch.jit.save(torch.jit.script(model_static_quantized), \"../models/PTSQ_VGG16.pt\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chohi/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::conv2d_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 72\u001B[0m\n\u001B[1;32m     69\u001B[0m     model_static_quantized(images\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m     71\u001B[0m model_static_quantized\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 72\u001B[0m model_static_quantized \u001B[38;5;241m=\u001B[39m \u001B[43mquantization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_static_quantized\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39msave(torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mscript(model_static_quantized), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/PTSQ_VGG16.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:657\u001B[0m, in \u001B[0;36mconvert\u001B[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    655\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inplace:\n\u001B[1;32m    656\u001B[0m     module \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(module)\n\u001B[0;32m--> 657\u001B[0m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    658\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    659\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    660\u001B[0m \u001B[43m    \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    661\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    663\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    664\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    665\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remove_qconfig:\n\u001B[1;32m    666\u001B[0m     _remove_qconfig(module)\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:714\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    707\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    708\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule)\n\u001B[1;32m    712\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping\n\u001B[1;32m    713\u001B[0m     ):\n\u001B[0;32m--> 714\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    718\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m            \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(\n\u001B[1;32m    723\u001B[0m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001B[1;32m    724\u001B[0m     )\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:714\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    707\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    708\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule)\n\u001B[1;32m    712\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping\n\u001B[1;32m    713\u001B[0m     ):\n\u001B[0;32m--> 714\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    718\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m            \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(\n\u001B[1;32m    723\u001B[0m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001B[1;32m    724\u001B[0m     )\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:722\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule)\n\u001B[1;32m    712\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping\n\u001B[1;32m    713\u001B[0m     ):\n\u001B[1;32m    714\u001B[0m         _convert(\n\u001B[1;32m    715\u001B[0m             mod,\n\u001B[1;32m    716\u001B[0m             mapping,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    720\u001B[0m             use_precomputed_fake_quant\u001B[38;5;241m=\u001B[39muse_precomputed_fake_quant,\n\u001B[1;32m    721\u001B[0m         )\n\u001B[0;32m--> 722\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m \u001B[43mswap_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_module_class_mapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    727\u001B[0m     module\u001B[38;5;241m.\u001B[39m_modules[key] \u001B[38;5;241m=\u001B[39m value\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:764\u001B[0m, in \u001B[0;36mswap_module\u001B[0;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    762\u001B[0m sig \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(qmod\u001B[38;5;241m.\u001B[39mfrom_float)\n\u001B[1;32m    763\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_precomputed_fake_quant\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m sig\u001B[38;5;241m.\u001B[39mparameters:\n\u001B[0;32m--> 764\u001B[0m     new_mod \u001B[38;5;241m=\u001B[39m \u001B[43mqmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_float\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\n\u001B[1;32m    766\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    768\u001B[0m     new_mod \u001B[38;5;241m=\u001B[39m qmod\u001B[38;5;241m.\u001B[39mfrom_float(mod)\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/conv.py:606\u001B[0m, in \u001B[0;36mConv2d.from_float\u001B[0;34m(cls, mod, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfrom_float\u001B[39m(\u001B[38;5;28mcls\u001B[39m, mod, use_precomputed_fake_quant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    600\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Creates a quantized module from a float module or qparams_dict.\u001B[39;00m\n\u001B[1;32m    601\u001B[0m \n\u001B[1;32m    602\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03m        mod (Module): a float module, either produced by torch.ao.quantization\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;124;03m          utilities or provided by the user\u001B[39;00m\n\u001B[1;32m    605\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 606\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConvNd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_float\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/conv.py:322\u001B[0m, in \u001B[0;36m_ConvNd.from_float\u001B[0;34m(cls, mod, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    320\u001B[0m         mod \u001B[38;5;241m=\u001B[39m mod[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    321\u001B[0m     weight_post_process \u001B[38;5;241m=\u001B[39m mod\u001B[38;5;241m.\u001B[39mqconfig\u001B[38;5;241m.\u001B[39mweight()\n\u001B[0;32m--> 322\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_qconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation_post_process\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_post_process\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/conv.py:255\u001B[0m, in \u001B[0;36m_ConvNd.get_qconv\u001B[0;34m(cls, mod, activation_post_process, weight_post_process)\u001B[0m\n\u001B[1;32m    253\u001B[0m qweight \u001B[38;5;241m=\u001B[39m _quantize_weight(mod\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mfloat(), weight_post_process)\n\u001B[1;32m    254\u001B[0m \u001B[38;5;66;03m# the __init__ call used is the one from derived classes and not the one from _ConvNd\u001B[39;00m\n\u001B[0;32m--> 255\u001B[0m qconv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_channels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_channels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    266\u001B[0m qconv\u001B[38;5;241m.\u001B[39mset_weight_bias(qweight, mod\u001B[38;5;241m.\u001B[39mbias)\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    268\u001B[0m     activation_post_process \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m activation_post_process\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat\n\u001B[1;32m    270\u001B[0m ):\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/conv.py:547\u001B[0m, in \u001B[0;36mConv2d.__init__\u001B[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001B[0m\n\u001B[1;32m    544\u001B[0m dilation \u001B[38;5;241m=\u001B[39m _pair(dilation)\n\u001B[1;32m    545\u001B[0m \u001B[38;5;66;03m# Subclasses of _ConvNd need to call _init rather than __init__. See\u001B[39;00m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;66;03m# discussion on PR #49702\u001B[39;00m\n\u001B[0;32m--> 547\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[43m    \u001B[49m\u001B[43min_channels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[43m    \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_pair\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfactory_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/conv.py:116\u001B[0m, in \u001B[0;36m_ConvNd._init\u001B[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001B[0m\n\u001B[1;32m     99\u001B[0m qweight \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_empty_affine_quantized(\n\u001B[1;32m    100\u001B[0m     weight_shape \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(kernel_size),\n\u001B[1;32m    101\u001B[0m     scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m factory_kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m    105\u001B[0m )\n\u001B[1;32m    106\u001B[0m bias_float \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    107\u001B[0m     torch\u001B[38;5;241m.\u001B[39mzeros(\n\u001B[1;32m    108\u001B[0m         out_channels,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    114\u001B[0m )\n\u001B[0;32m--> 116\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_weight_bias\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias_float\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzero_point \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/conv.py:567\u001B[0m, in \u001B[0;36mConv2d.set_weight_bias\u001B[0;34m(self, w, b)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mset_weight_bias\u001B[39m(\u001B[38;5;28mself\u001B[39m, w: torch\u001B[38;5;241m.\u001B[39mTensor, b: Optional[torch\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 567\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantized\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d_prepack\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m            \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    571\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mquantized\u001B[38;5;241m.\u001B[39mconv2d_prepack(\n\u001B[1;32m    572\u001B[0m             w, b, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups\n\u001B[1;32m    573\u001B[0m         )\n",
      "File \u001B[0;32m~/project/ai/DeepLearningDeepen/DeepLearningAdv/.venv/lib/python3.9/site-packages/torch/_ops.py:1123\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_torchbind_op_overload \u001B[38;5;129;01mand\u001B[39;00m _must_dispatch_in_python(args, kwargs):\n\u001B[1;32m   1122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _call_overload_packet_from_python(\u001B[38;5;28mself\u001B[39m, args, kwargs)\n\u001B[0;32m-> 1123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Didn't find engine for operation quantized::conv2d_prepack NoQEngine"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5f24512c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T06:25:11.538025Z",
     "start_time": "2025-03-29T06:25:08.907300Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# 사전 훈련된 VGG16 모델 로드\n",
    "#vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "vgg16 = models.vgg16(num_classes=2)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#model.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "# 평가 모드로 설정\n",
    "vgg16.eval()\n",
    "\n",
    "# 모델을 'VGG16.pt'로 저장\n",
    "#torch.save(vgg16.state_dict(), 'VGG16.pt')\n",
    "torch.save(vgg16.state_dict(), \"../models/VGG16.pt\")\n",
    "\n",
    "\n",
    "print(\"VGG16 모델이 VGG16.pt로 저장되었습니다.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 모델이 VGG16.pt로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
