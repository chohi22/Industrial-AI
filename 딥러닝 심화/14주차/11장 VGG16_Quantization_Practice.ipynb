{
 "cells": [
  {
   "cell_type": "code",
   "id": "f49f0bd8-df62-4097-9d3e-9378e48c8e9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f49f0bd8-df62-4097-9d3e-9378e48c8e9b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1749100364496,
     "user_tz": -540,
     "elapsed": 53341,
     "user": {
      "displayName": "Youngbae Hwang",
      "userId": "03946825880639667856"
     }
    },
    "outputId": "faa7ddb2-53b4-4b1e-a599-a93ea3d8e516",
    "ExecuteTime": {
     "end_time": "2025-06-05T14:04:09.663530Z",
     "start_time": "2025-06-05T14:04:08.444624Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.ao import quantization\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "class QuantizedVGG16(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedVGG16, self).__init__()\n",
    "        self.quant = quantization.QuantStub()\n",
    "        self.dequant = quantization.DeQuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "hyperparams = {\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"epochs\": 5,\n",
    "    \"transform\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48235, 0.45882, 0.40784],\n",
    "                std=[1.0 / 255.0, 1.0 / 255.0, 1.0 / 255.0],\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "model = models.vgg16(num_classes=2)\n",
    "model.load_state_dict(torch.load(\"./models/VGG16.pt\"))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantized_model = QuantizedVGG16(model).to(device)\n",
    "\n",
    "quantization_backend = \"fbgemm\"\n",
    "quantized_model.qconfig = quantization.get_default_qconfig(quantization_backend)\n",
    "model_static_quantized = quantization.prepare(quantized_model)\n",
    "\n",
    "calibartion_dataset = ImageFolder(\n",
    "    \"./datasets/pet/test\",\n",
    "    transform=hyperparams[\"transform\"]\n",
    ")\n",
    "calibartion_dataloader = DataLoader(\n",
    "    calibartion_dataset,\n",
    "    batch_size=hyperparams[\"batch_size\"]\n",
    ")\n",
    "\n",
    "for i, (images, target) in enumerate(calibartion_dataloader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    model_static_quantized(images.to(device))\n",
    "\n",
    "model_static_quantized.to(\"cpu\")\n",
    "model_static_quantized = quantization.convert(model_static_quantized)\n",
    "\n",
    "torch.jit.save(torch.jit.script(model_static_quantized), \"./models/PTSQ_VGG16.pt\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2387395/2502900318.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./models/VGG16.pt\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported qscheme: per_channel_affine",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 65\u001B[0m\n\u001B[1;32m     62\u001B[0m     model_static_quantized(images\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m     64\u001B[0m model_static_quantized\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 65\u001B[0m model_static_quantized \u001B[38;5;241m=\u001B[39m \u001B[43mquantization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_static_quantized\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39msave(torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mscript(model_static_quantized), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./models/PTSQ_VGG16.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/quantization/quantize.py:563\u001B[0m, in \u001B[0;36mconvert\u001B[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    561\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inplace:\n\u001B[1;32m    562\u001B[0m     module \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(module)\n\u001B[0;32m--> 563\u001B[0m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remove_qconfig:\n\u001B[1;32m    568\u001B[0m     _remove_qconfig(module)\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/quantization/quantize.py:604\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    601\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    603\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[0;32m--> 604\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    605\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m                 \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/quantization/quantize.py:604\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    601\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    603\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[0;32m--> 604\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    605\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m                 \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/quantization/quantize.py:607\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    603\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[1;32m    604\u001B[0m         _convert(mod, mapping, \u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# inplace\u001B[39;00m\n\u001B[1;32m    605\u001B[0m                  is_reference, convert_custom_config_dict,\n\u001B[1;32m    606\u001B[0m                  use_precomputed_fake_quant\u001B[38;5;241m=\u001B[39muse_precomputed_fake_quant)\n\u001B[0;32m--> 607\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m \u001B[43mswap_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_module_class_mapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    610\u001B[0m     module\u001B[38;5;241m.\u001B[39m_modules[key] \u001B[38;5;241m=\u001B[39m value\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/quantization/quantize.py:642\u001B[0m, in \u001B[0;36mswap_module\u001B[0;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    640\u001B[0m sig \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(qmod\u001B[38;5;241m.\u001B[39mfrom_float)\n\u001B[1;32m    641\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_precomputed_fake_quant\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m sig\u001B[38;5;241m.\u001B[39mparameters:\n\u001B[0;32m--> 642\u001B[0m     new_mod \u001B[38;5;241m=\u001B[39m \u001B[43mqmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_float\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    644\u001B[0m     new_mod \u001B[38;5;241m=\u001B[39m qmod\u001B[38;5;241m.\u001B[39mfrom_float(mod)\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/nn/quantized/modules/conv.py:480\u001B[0m, in \u001B[0;36mConv2d.from_float\u001B[0;34m(cls, mod, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfrom_float\u001B[39m(\u001B[38;5;28mcls\u001B[39m, mod, use_precomputed_fake_quant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    474\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Creates a quantized module from a float module or qparams_dict.\u001B[39;00m\n\u001B[1;32m    475\u001B[0m \n\u001B[1;32m    476\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;124;03m        mod (Module): a float module, either produced by torch.ao.quantization\u001B[39;00m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;124;03m          utilities or provided by the user\u001B[39;00m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConvNd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_float\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_precomputed_fake_quant\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/nn/quantized/modules/conv.py:242\u001B[0m, in \u001B[0;36m_ConvNd.from_float\u001B[0;34m(cls, mod, use_precomputed_fake_quant)\u001B[0m\n\u001B[1;32m    240\u001B[0m         mod \u001B[38;5;241m=\u001B[39m mod[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    241\u001B[0m     weight_post_process \u001B[38;5;241m=\u001B[39m mod\u001B[38;5;241m.\u001B[39mqconfig\u001B[38;5;241m.\u001B[39mweight()\n\u001B[0;32m--> 242\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_qconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation_post_process\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_post_process\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/nn/quantized/modules/conv.py:209\u001B[0m, in \u001B[0;36m_ConvNd.get_qconv\u001B[0;34m(cls, mod, activation_post_process, weight_post_process)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;66;03m# the __init__ call used is the one from derived classes and not the one from _ConvNd\u001B[39;00m\n\u001B[1;32m    206\u001B[0m qconv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(mod\u001B[38;5;241m.\u001B[39min_channels, mod\u001B[38;5;241m.\u001B[39mout_channels, mod\u001B[38;5;241m.\u001B[39mkernel_size,\n\u001B[1;32m    207\u001B[0m             mod\u001B[38;5;241m.\u001B[39mstride, mod\u001B[38;5;241m.\u001B[39mpadding, mod\u001B[38;5;241m.\u001B[39mdilation, mod\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[1;32m    208\u001B[0m             mod\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, mod\u001B[38;5;241m.\u001B[39mpadding_mode)\n\u001B[0;32m--> 209\u001B[0m \u001B[43mqconv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_weight_bias\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m activation_post_process \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m activation_post_process\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat:\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m qconv  \u001B[38;5;66;03m# dynamic quantization doesn't need scale/zero_point\u001B[39;00m\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/ao/nn/quantized/modules/conv.py:445\u001B[0m, in \u001B[0;36mConv2d.set_weight_bias\u001B[0;34m(self, w, b)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mset_weight_bias\u001B[39m(\u001B[38;5;28mself\u001B[39m, w: torch\u001B[38;5;241m.\u001B[39mTensor, b: Optional[torch\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 445\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantized\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d_prepack\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    446\u001B[0m \u001B[43m            \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    448\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mquantized\u001B[38;5;241m.\u001B[39mconv2d_prepack(\n\u001B[1;32m    449\u001B[0m             w, b, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "File \u001B[0;32m~/project/.venv/lib64/python3.8/site-packages/torch/_ops.py:1061\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self_, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m self_\u001B[38;5;241m.\u001B[39m_has_torchbind_op_overload \u001B[38;5;129;01mand\u001B[39;00m _must_dispatch_in_python(args, kwargs):\n\u001B[1;32m   1060\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001B[0;32m-> 1061\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mself_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Unsupported qscheme: per_channel_affine"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.48235, 0.45882, 0.40784],\n",
    "            std=[1.0 / 255.0, 1.0 / 255.0, 1.0 / 255.0],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image = Image.open(\"./datasets/images/cat.jpg\")\n",
    "inputs = transform(image).unsqueeze(0)\n",
    "\n",
    "model = models.vgg16(num_classes=2)\n",
    "model.load_state_dict(torch.load(\"./models/VGG16.pt\"))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_static_quantized = torch.jit.load(\"./models/PTSQ_VGG16.pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    outputs = model(inputs.to(device))\n",
    "    file_size = os.path.getsize(\"./models/VGG16.pt\") / 1e6\n",
    "    print(\"양자화 적용 전:\")\n",
    "    print(f\"출력 결과: {outputs}\")\n",
    "    print(f\"추론 시간: {time.time() - start_time:.4f}s\")\n",
    "    print(f\"파일 크기: {file_size:.2f} MB\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = model_static_quantized(inputs)\n",
    "file_size = os.path.getsize(\"./models/PTSQ_VGG16.pt\") / 1e6\n",
    "end_time = time.time() - start_time\n",
    "print(\"양자화 적용 후:\")\n",
    "print(f\"출력 결과: {outputs}\")\n",
    "print(f\"추론 시간: {time.time() - start_time:.4f}s\")\n",
    "print(f\"파일 크기: {file_size:.2f} MB\")"
   ],
   "metadata": {
    "id": "qbqK1OqKhhPh",
    "ExecuteTime": {
     "end_time": "2025-06-05T14:03:48.552696Z",
     "start_time": "2025-06-05T14:03:47.350470Z"
    }
   },
   "id": "qbqK1OqKhhPh",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2387395/693146444.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./models/VGG16.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양자화 적용 전:\n",
      "출력 결과: tensor([[ 25.6511, -22.3752]], device='cuda:0')\n",
      "추론 시간: 0.0027s\n",
      "파일 크기: 537.08 MB\n",
      "\n",
      "\n",
      "양자화 적용 후:\n",
      "출력 결과: tensor([[ 26.0821, -22.6373]])\n",
      "추론 시간: 0.0415s\n",
      "파일 크기: 134.55 MB\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "1psuW1ckQ6Nnzv2OOxeV7w65farrMQ0AX",
     "timestamp": 1749100692851
    },
    {
     "file_id": "1j6F1ox6me_pYDIf3XLh9h_AGji2DVTZs",
     "timestamp": 1749100635912
    }
   ],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
