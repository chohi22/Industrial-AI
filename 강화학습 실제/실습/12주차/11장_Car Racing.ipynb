{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### car racing - torch",
   "id": "94b0d1c34212157e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T01:44:32.077352Z",
     "start_time": "2025-05-22T01:44:32.061817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import pylab as p\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def get_batch(self):\n",
    "        data = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        state = np.stack([x[0] for x in data])\n",
    "        action = np.array([x[1] for x in data])\n",
    "        reward = np.array([x[2] for x in data])\n",
    "        next_state = np.stack([x[3] for x in data])\n",
    "        done = np.array([x[4] for x in data]).astype(np.int32)\n",
    "\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "\n",
    "\n",
    "# CNN 특성 추출기\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # 선형 레이어는 나중에 초기화\n",
    "        self.feature_size = None\n",
    "        self.linear = None\n",
    "        self.initialized = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"CNNFeatureExtractor 입력 형태: {x.shape}\")\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(f\"conv1 후 형태: {x.shape}\")\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(f\"conv2 후 형태: {x.shape}\")\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(f\"conv3 후 형태: {x.shape}\")\n",
    "\n",
    "        # 평탄화 (1D로)\n",
    "        batch_size = x.size(0)\n",
    "        flat_x = x.view(batch_size, -1)\n",
    "        #print(f\"평탄화 후 형태: {flat_x.shape}\")\n",
    "\n",
    "        # 첫 번째 실행 시 특성 크기 감지 및 레이어 초기화\n",
    "        if not self.initialized:\n",
    "            self.feature_size = flat_x.size(1)\n",
    "            #print(f\"특성 크기 감지: {self.feature_size}\")\n",
    "            self.linear = nn.Linear(self.feature_size, 128)\n",
    "            if x.is_cuda:\n",
    "                self.linear = self.linear.cuda()\n",
    "            self.initialized = True\n",
    "\n",
    "        x = F.relu(self.linear(flat_x))\n",
    "        return x\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(QNet, self).__init__()\n",
    "        self.feature = CNNFeatureExtractor()\n",
    "\n",
    "        # 나머지 레이어들\n",
    "        self.l1 = nn.Linear(128, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"QNet 입력 형태: {x.shape}\")\n",
    "\n",
    "        x = self.feature(x)\n",
    "        #print(f\"특성 추출 후 형태: {x.shape}\")\n",
    "\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ],
   "id": "dc319cc97ffb31fd",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T01:34:27.920015Z",
     "start_time": "2025-05-22T01:34:27.916646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_model_structure(model):\n",
    "    print(\"📐 Neural Network Structure:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(model)\n",
    "    print(\"=\" * 50)"
   ],
   "id": "20168a7fb29e9b6d",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-22T01:44:42.653661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.0005\n",
    "        self.epsilon = 0.1\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 32\n",
    "        self.action_size = 5\n",
    "        self.device = device\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "        self.qnet = QNet(self.action_size).to(self.device)\n",
    "        self.qnet_target = QNet(self.action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.qnet.parameters(), lr=self.lr)\n",
    "\n",
    "        #print_model_structure(self.qnet)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            state = state[np.newaxis, :]\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # # state = torch.FloatTensor(state)\n",
    "            # # state = state.permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "            # print(f\"원본 state 형태: {state.shape}\")\n",
    "            #\n",
    "            # # 상태 전처리\n",
    "            # #state = preprocess(state)\n",
    "            #\n",
    "            # print(state.shape)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                qs = self.qnet(state)\n",
    "            return qs.argmax(dim=1).item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.get_batch()\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        action = torch.tensor(action, dtype=torch.long).to(self.device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32).to(self.device)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "        done = torch.tensor(done, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        qs = self.qnet(state)\n",
    "        q = qs[torch.arange(self.batch_size), action]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_qs = self.qnet_target(next_state)\n",
    "            next_q = next_qs.max(1)[0]\n",
    "            target = reward + (1 - done) * self.gamma * next_q\n",
    "\n",
    "        loss = F.mse_loss(q, target)\n",
    "\n",
    "        self.qnet.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def sync_qnet(self):\n",
    "        self.qnet_target.load_state_dict(self.qnet.state_dict())\n",
    "\n",
    "\n",
    "def preprocess(state):\n",
    "    #state = torch.FloatTensor(state)\n",
    "    state = np.transpose(state, (2, 0, 1))\n",
    "    state = state / 255.0\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "episodes = 1000\n",
    "sync_interval = 10\n",
    "\n",
    "\n",
    "env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "agent = DQNAgent(device=device)\n",
    "\n",
    "# Print observation and action space\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    state = preprocess(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = preprocess(next_state)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if episode % sync_interval == 0:\n",
    "        agent.sync_qnet()\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode} | Total Reward {total_reward}\")\n",
    "\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(range(len(reward_history)).reward_history)\n",
    "plt.show()"
   ],
   "id": "1ef280efcf2a42cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Action Space: Discrete(5)\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
